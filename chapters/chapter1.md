#### 第一章 可靠的可扩展的可维护的应用

今天的许多应用是数据密集型的，与之相反的是计算密集型的。cpu的计算能力并不是这类应用的瓶颈－难点其实是数据量，数据的错综复杂，以及数据更新的速度。

一个典型的数据密集应用常由一些标准模块构成。例如，许多应用需要：

* 存储数据，方便他们自己或者其他应用稍后使用（数据库）
* 记住比较耗时的操作结果，提高读的速度（缓存）
* 允许用户通过关键字搜索或者通过多种方式过滤（搜索索引）
* 发送一个消息到其他进程，异步处理消息（消息队列）
* 观察事件发生，并且当事件发生时执行相应的操作（流式处理）
* 周期性的计算大量的积聚的数据（批处理）

如果这听起来很明显，这是因为这些数据系统是一个合适的抽象：我们一直在使用这些系统。当构建一个应用，大多数工程师不会从头开始自己写一个数据存储引擎，因为数据库是完成这个工作的最完美的工具。

但是现实并没有那么简单。有很多不同特性的数据库，因为不同的应用其需求也不同。有很多方式缓存，多种方法构建查询索引，等等。当构建一个应用时，我们仍然需要明确指出那种工具哪种方法适合我们手上的工作.有时候当单独使用一种简单的工具并不能完成此类工作时，组合使用多种工具完成特定任务很有难度的。

这本书主要结束数据系统的原则和实践经验，并且如何如何使用这些工具构建数据密集的应用。我们同样探讨了不同工具之间的共同点，他们的区别，以及这些工具是如何达到他们所具有的特点的。

#####数据系统的思考

一般的我们认为数据库，队列，缓存，数据不同的工具种类。虽然一个数据库和一个队列表面上有很多相似点－都存储一段时间的数据－在访问模式上有很的不同，也就是说不同的性能特性，因此也有不同的实现方式。

那为什么要把他们归并到数据系统一类呢？

近些年来涌现出了许多新的数据存储处理工具。他们针对特定的应用情形做了很多优化，他们不再很好的归类到这些种类当中。例如，有些数据存储工具用来当消息队列，有些消息队列又有类似数据库对数据持久化的功能（kafka），所以这些门类的边界变的模糊不清。

第二点，很多应用有这样的硬性需求或者广泛要求，这些要求单一的工具并不能胜任数据处理和存储的需求。取而代之的是，这种工作分解成多个任务，这些小的任务单个工具可以很好的完成，使用应用代码将这些小任务组装起来实现其功能需求。

例如，如果你有一个应用管理的缓存层（使用memecached 或者类似的），或者和主数据库相隔离的全文索引搜索服务器（es or solr）,保持缓存索引和主数据库保持一致则是业务代码所要完成的工作。figure1-1说明了这种系统的大概情况。

当你为了提供一个服务，一个服务接口，或者api使得客户端对其service实现是透明的，自己家需要组合多种工具。现在你已经通过一些小的通用性的组件构建了一个新特定需要的数据系统。你构建的系统需要提供一定的保证，例如缓存再写时要正确的失效或者更新，以致于外面的客户端看到一致的结果。你现在并不仅仅是一个应用开发者，还是一个数据系统设计者。

如果你设计一个数据系统或者服务，会遇到很多复杂的问题。当系统内部出现错误时如何保证数据保存的准确性和完整性。即使部分系统挂掉了能否持续的为客户端提供好的服务？如何通过扩展来应对服务负载的上升？一个好的api服务应该是什么样的？有很多因此可以影响你的系统设计，包括参与设计的人的技能和经验，历史系统的依赖性，__组织应对不同风险的容忍度以及监督的限制__。


这本书，集中讨论在软件系统种重点关心的三个方面：

* 可靠性

		系统在面对任何问题（软件硬件错误，甚至人为错误）时可以继续正确的工作（在可接受的性能下正确的完成工作）

* 可扩展性

        当系统增长（数据量，流量或者复杂度），应该有合理的方法应对这种增长
        
* 可维护性

		随着时间的推移，许多不同的人会在这个系统上工作（工程师，运维，保持当前的行为并且采用该系统满足新的需求），他们都可以在该系统上有效率的工作
		
这些词到处传播，但却不知道其真正含义。在这一章的后半部分将探讨可靠性、可扩展性、和可维护性。接着，在接下来的几个章节中，将主要研究几个不同技术架构以及为了实现这些目标所使用的算法。

##### 可靠性

对于软件的可靠性或者不可靠性每个人都有一个直观的认识。对于软件来说，包括：

* 应用符合用户预期的工作
* 应用可以容忍用户错误使用或者人为的错误
* 在预期的负载或者数据量下，其性能符合用户需求
* 系统阻止非授权用户使用

如果这些东西合起来意味着“正确的工作”,那么我们可以理解可靠性为"即使出问题了仍然可以正常工作"。

可以出错的事情称之为错误，系统可以预料到错误并且可以处理它称之为容错性。 这个词有些误导： 建议我们应该构建一个系统，它可以容忍任何错误，有些错误在现实中其实并不可能。所以只考虑容忍一定类型的错误。

错误并不是失败。 一个错误通常定义为违背系统规则的一部分，而错误是系统作为整体并不能正确的提供服务。将错误的概率降低到0是不可能的，因此设计一个错误容忍的机制是防止错误引起失败的最好实践。这本书主要涵盖构建可靠系统的几个技术。

凭直觉的，在这样的错误容忍的系统中，故意的触发这些错误增加错误的概率是有意义的。故意的造成错误，例如没有语境的前提下杀掉独立的进程 －Chaos Monkey（Netflix刚刚开源了他们那被人惦记好一阵子的“Chaos Monkey”，这是一套用来故意把服务器搞下线的软件，可以测试云环境的恢复能力。）。保证错误容忍机制不断的测试，所以当错误自然的发生我们可以相信这些错误可以正确的处理。

虽然我们通常喜欢容忍错误而不是避免错误，但是有些情况避免错误要比容忍错误要好（因为没有解决方法）。安全问题就是这样一个情况，例如：如果一个攻击已经挟持了一个系统并且获得访问敏感数据的权限，这种事情不能消除。然而这本书主要考虑的是那些下面所介绍的可以解决的错误。

###### 硬件故障

当我们考虑引起系统失败的原因时，硬件故障是第一考虑的。硬盘坏了，内存错误，机房断电，有些了拔掉了错误的网络电缆。任何有过大数据中心工作的人会告诉你当你有大量服务器时，这些事情一直在发生。

据报道，硬盘平均10-50年就会发生故障。因此，在一个拥有10000硬盘的存储集群中，平均一天就会有一个磁盘挂掉。

为了减少系统的故障率我们的第一个反应就是为单独的硬件组件增加冗余。磁盘可以设置为RAID，服务器可能有双核和热插拔的cpu，数据中心有电池和柴油发电机的备用电源。当一个组件挂了，冗余的组件可以代替出问题的组件工作。这种方法不能完全防止由于硬件问题造成的故障，但是这种方法可以保证在不干预的情况下工作几年。

直到现在，硬件组件冗余对于大多数应用来时足够了，因为这造成单哥机子故障是很少发生的。只要能在新机子上短时间内恢复备份，由于这种故障造成的宕机对于大多数应用来说并不是灾难性的。因此多机冗余只有在那些高可用性要求的应用中才会需要。

然而，由于数据量和应用计算需求的增加，越来越多的应用需要大量的机子，同样潜在的增加了硬件故障率。而且，在一些类似亚马逊web服务的云平台，对于虚拟机实例没有任何警告的情况下服务不可用是很平常的，因此平台为了有利于单机的可靠性设计为高灵活和有弹性的。

因此在容忍整个物理机故障的系统中，与硬件冗余对比，使用软件容忍错误的技术是一大进步。这样的系统同样拥有操作的优势：单服务器的系统如果需要重启服务器需要详细的下线计划，而容忍机器故障的系统可用一次下线一个机子，而不用停止当前系统的运行。

###### 软件错误

我们通常认为硬件故障是随机的且相互之间是独立的：一个服务器的磁盘坏了并不会影响其他机器的硬盘。 他们之间有比较弱的相关性，但是大量的硬件组件同时发生故障是不太可能发生。

另一类错误是系统中得系统错误。这类故障很难预料，并且跨机之间相互有关系，相比较不相关的硬件故障，这类错误会导致更多的系统的错误。例如：

* 当应用输入错误的数据，系统bug会引起系统的每个应用服务器宕掉。例如，考虑一个linux内核的bug，2012 年6月 30号的一秒的跳动造成很多应用同时挂掉。
* 失控的进程占耗尽了共享资源-cpu时钟时间，内存，硬盘空间和网络带宽
* 系统依赖的服务变慢，变的不能相应。
* 链式错误即一个异常导致灵感一个组件的错误，然后又会导致另一个异常。

这类bug一直潜伏直到触发该bug的不常见的情形发生。在这些情况，反映出软件对环境做了一些假设，通常情况下这些假设是true，由于一些原因这些假设不再成立。

在软件中这类系统错误没有快速的解决方案。许多细节可以起到作用：仔细考虑系统中得这些假设和交互。彻底的测试，测量监控和分析生产环境的系统行为。如果一个系统需要吸氧保证（例如，在消息队列中，输入的队列个数等于输出的消息的个数），当系统运行时可以不停的检查自己，如果存在差异马上报警。

###### 人为错误

人设计并构建了软件系统，维护系统运行的操作都是人。即使他们有很好的意图，人类依然认为不太可靠。尽管人类是不可靠的，怎样使我们的系统变的可靠。

最好的系统又几个方法：

* 以减少错误的机会设计系统。例如，一个好得设计抽象，api 和管理接口使得很容易做对的事，阻止做坏得事情。然而，如果接口太严格，人们需要研究这些接口，需要权衡利弊
* 分离人们容易出错并且引起的故障的地方。特别的，提供一个全部特性非生产环境的沙盒环境，这个环境使用真实数据，但不会影响到真正用户，使得人们可以在上面安全的随便使用测试，
* 所有级别的测试，从单元测试到系统集成测试在逃人工测试。自动测试广泛使用，很好理解，并且可以覆盖到常规操作很少见的情形。
* 允许快速简单的从人为错误中恢复，减少错误情形的影响。例如，配置文件的修改快速回滚，逐渐的上线新代码。提供工具重新计算数据。
* 设定详细的明确的监控，例如性能指标和错误率。监控可以提前告诉我们预警信号，允许我们检查任何假设和常量是否有违背。当一个错误发生时，指标在诊断错误并不会有任何价值。
* 很好的管理实践，培训

######可靠性是多么的重要

可靠性并不是只有核电站和空中交通控制软件才有-更多的平常的应用也期望可靠的工作。在商业应用中的bug会减低其生产率，商业网站的运行中断会造成资金声誉的巨大损失。

即使在不重要的应用中对于用户我们也有义务。考虑一下一位父亲存了在你的应用中他还在的所有照片视频，当数据库突然宕机他们的感觉是怎么样的？他们知道如何恢复吗？

也有一些情形我们牺牲可靠性来降低开发成本。但我们必须清醒的知道什么时候可以这样做。

#####可扩展性

即使一个系统今天可靠的运行，这并不意味着将来仍然可以正确的工作。服务变差的一个常见原因是负载的增加：可能从10000并发用户增加到100000并发用户，从10完到100万。或者处理比以往更大的数据量。

可扩展这一词我们用来描述系统自适应增长的负载的能力。注意，这并不是只可以扩展系统的一个维度，如果说x可以扩展y不可以扩展是毫无意义的。进一步，讨论扩展性意味着讨论这个问题:如果系统以一种特定的方式增长，那我们应对这种增长的可选项是什么？

###### 描述负载
 
 首先简单的描述一下系统的负载；只有那时候我们才讨论增长问题（如果负载翻倍了发生什么了）。负载可以用一些负载参数来描述。参数中最好的选择要依赖特定的系统架构: 可能是qps，读写比率，并发用户，或者其他。或者是一些你头疼的问题，或者是少数的特例造成的瓶颈。
 
 使这些概念更加精确，考虑一些twitter这个例子，这里的数据使用的是2012 11月的数据。Twitter主要操作如下
 
 * 发表tweet
 
 		一个用户可以向他得粉丝发表新消息（每秒4.6k个请求，高峰期达到每秒12k）
 		
 * 主页的时间轴
 		
 		用户可以看到关注者最近发表的消息（每秒30万次）
 
简单的处理12k次写操作每秒很简单。但是Twitter的可扩展挑战并不是数据量，而是它的fan-out: m每个用户关注很多人，每个用户被很多人关注。大体上有两种方法实现这两个操作:
 
1. 发表一个新的消息只是简单的在一个全局的消息集合中插入一条记录。 












